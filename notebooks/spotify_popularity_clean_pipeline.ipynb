{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "plaintext"
        }
      },
      "outputs": [],
      "source": [
        "# Spotify Popularity Prediction â€” Clean Pipeline\n",
        "\n",
        "Notebook with a structured, presentation-ready workflow:\n",
        "- Imports & setup\n",
        "- Load data\n",
        "- EDA (distribution + correlations)\n",
        "- Robust preprocessing (numeric/categorical)\n",
        "- Random Forest with small grid search\n",
        "- Evaluation (metrics + plots)\n",
        "- Feature importances\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1) Imports & Setup\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "sns.set(style=\"whitegrid\", context=\"talk\")\n",
        "plt.rcParams[\"figure.dpi\"] = 120\n",
        "\n",
        "DATASET_PATH = Path(\"/Users/anooptejthotapalli/Downloads/dataset-3-1.csv\")\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2\n",
        "\n",
        "assert DATASET_PATH.exists(), f\"Dataset not found at {DATASET_PATH}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2) Load Data\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(\"Data shape:\", df.shape)\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3) Light Cleaning & EDA\n",
        "# Keep a modeling copy\n",
        "df_model = df.copy()\n",
        "\n",
        "# Convert boolean to int if present\n",
        "if 'explicit' in df_model.columns:\n",
        "    df_model['explicit'] = df_model['explicit'].astype(int)\n",
        "\n",
        "# EDA\n",
        "numeric_cols_eda = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "sns.histplot(df['popularity'], bins=30, kde=True, ax=ax[0])\n",
        "ax[0].set_title('Popularity Distribution')\n",
        "ax[0].set_xlabel('popularity'); ax[0].set_ylabel('count')\n",
        "\n",
        "sns.heatmap(df[numeric_cols_eda].corr(), cmap='coolwarm', center=0, ax=ax[1])\n",
        "ax[1].set_title('Correlation Heatmap (numeric)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4) Features/Target & Column Types\n",
        "assert 'popularity' in df_model.columns, \"Target 'popularity' not found.\"\n",
        "X = df_model.drop(columns=['popularity', 'track_id', 'artists', 'album_name', 'track_name'], errors='ignore')\n",
        "y = df_model['popularity']\n",
        "\n",
        "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
        "\n",
        "print('Numeric features:', len(numeric_features))\n",
        "print('Categorical features:', len(categorical_features))\n",
        "list(numeric_features)[:10]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5) Preprocessing & Model Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "numeric_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='mean')),\n",
        "        ('scaler', StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer = Pipeline(\n",
        "    steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('ohe', OneHotEncoder(handle_unknown='ignore')),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features),\n",
        "    ],\n",
        "    remainder='drop',\n",
        ")\n",
        "\n",
        "pipe = Pipeline(\n",
        "    steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('model', RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\n",
        "    'model__n_estimators': [100, 200],\n",
        "    'model__max_depth': [None, 8, 16],\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring='neg_mean_squared_error',\n",
        "    cv=3,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "print('Best Params:', grid.best_params_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6) Evaluation\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "y_pred = best_model.predict(X_test)\n",
        "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Random Forest RMSE: {rmse:.2f}\")\n",
        "print(f\"Random Forest R^2:  {r2:.2f}\")\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 5))\n",
        "# Pred vs Actual\n",
        "ax[0].scatter(y_test, y_pred, alpha=0.3)\n",
        "lims = [min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max())]\n",
        "ax[0].plot(lims, lims, 'r--')\n",
        "ax[0].set_title('Predicted vs Actual')\n",
        "ax[0].set_xlabel('Actual popularity')\n",
        "ax[0].set_ylabel('Predicted popularity')\n",
        "\n",
        "# Residuals\n",
        "residuals = y_test - y_pred\n",
        "sns.histplot(residuals, bins=30, kde=True, ax=ax[1])\n",
        "ax[1].set_title('Residuals Distribution')\n",
        "ax[1].set_xlabel('Residual (Actual - Predicted)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7) Feature Importances\n",
        "from typing import List\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "def get_feature_names_from_column_transformer(ct: ColumnTransformer) -> List[str]:\n",
        "    feature_names = []\n",
        "    for name, transformer, cols in ct.transformers_:\n",
        "        if name == 'remainder' and transformer == 'drop':\n",
        "            continue\n",
        "        if hasattr(transformer, 'named_steps'):\n",
        "            last = list(transformer.named_steps.values())[-1]\n",
        "            if hasattr(last, 'get_feature_names_out'):\n",
        "                fn = last.get_feature_names_out(cols)\n",
        "                feature_names.extend(fn)\n",
        "            else:\n",
        "                feature_names.extend(cols)\n",
        "        else:\n",
        "            feature_names.extend(cols if isinstance(cols, list) else [cols])\n",
        "    return [str(f) for f in feature_names]\n",
        "\n",
        "pre = best_model.named_steps['preprocessor']\n",
        "rf = best_model.named_steps['model']\n",
        "\n",
        "feature_names = get_feature_names_from_column_transformer(pre)\n",
        "importances = pd.Series(rf.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "\n",
        "TOP_N = 15\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.barplot(x=importances.head(TOP_N).values, y=importances.head(TOP_N).index, orient='h')\n",
        "plt.title(f'Top {TOP_N} Feature Importances')\n",
        "plt.xlabel('Importance'); plt.ylabel('Feature')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print('Top 5 features:\\n', importances.head(5))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
